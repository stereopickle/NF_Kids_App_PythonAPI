{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symptom Identifier\n",
    "This document contains exploration to build the NLP model to determine probability or similarity of how much given text indicates each symptom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import joblib\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia as wiki\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import bigrams\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Symptoms\n",
    "Now I will write some simple nlp steps to extract symptoms from descriptions. Until we collect enough text data, we will rely on the vector space provided by Spacy  to determine how closely a description is related to the each symptoms. We'll also see if the public domain medical transcription data will create a better vector space to identify the symptoms from descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing symptoms\n",
    "symptoms = pd.read_json('data/symptoms.json', orient = 'table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "first, preprocessing steps for text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text, punctuations = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~�0123456789®'):\n",
    "    ''' remove punctuations '''\n",
    "    table_ = str.maketrans(punctuations, ' '*len(punctuations))\n",
    "    return text.translate(table_)\n",
    "\n",
    "def ascii_only(text):\n",
    "    ''' remove non-ascii words '''\n",
    "    return text.encode(\"ascii\", \"ignore\").decode()\n",
    "\n",
    "def lemmatize(word):\n",
    "    ''' lemmatize text'''\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return wnl.lemmatize(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Corpus\n",
    "Initially, we will build the corpus using the relevant Wikipedia page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(pages, corpus = set()):\n",
    "    print(f\"Reading {len(pages)} pages...\")\n",
    "    for pg in pages: \n",
    "        print(pg, end = '')\n",
    "        try:\n",
    "            print(' | ', end = '')\n",
    "            soup = BeautifulSoup(wiki.page(pg).html(), 'html.parser')\n",
    "        except:\n",
    "            print(' (error) | ', end = '')\n",
    "            continue\n",
    "        text = re.sub('\\n', ' ', soup.get_text())\n",
    "        text = re.sub(\"\\[[^\\[\\]]*\\]\", \" \", text)\n",
    "        text = re.sub(\"{[^{}]*}\", \"\", text)\n",
    "        text = re.sub(\"\\.[a-z0-9-]+\", \"\", text)\n",
    "        text = re.sub(\"References  .+\", \"\", text)\n",
    "        text = re.sub(r\"([^A-Z-(])([A-Z])\", r\"\\1 \\2\", text)\n",
    "        text = re.sub(\"\\xa0[0-9]*\", \" \", text)\n",
    "        text = remove_punctuations(text, punctuations = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~�®')\n",
    "        words = set(x.lower() for x in text.split() if len(x) > 1)\n",
    "        corpus = corpus | words\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_corpus(search_term, max_pages, corpus = set()):\n",
    "    pages = wiki.search(search_term, results=max_pages)\n",
    "    current_length = len(corpus)\n",
    "    new_corpus = get_words(pages, corpus)\n",
    "    print(f\"Added {len(new_corpus) - current_length} words: Total {len(new_corpus)} words\")\n",
    "    return new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 100 pages...\n",
      "Neurofibromatosis | Neurofibromatosis type I | Neurofibromatosis type II | Café au lait spot | Legius syndrome | Gillian Anderson |  (error) | Neurofibroma | Genetic disorder | Neurofibromatosis type 3 | Watson syndrome | Malignant peripheral nerve sheath tumor | Adam Pearson (actor) | Neurofibromin 1 | Noonan syndrome | Lisch nodule | Neurofibromin | Glioblastoma | Scoliosis | Phakomatosis | Crowe sign | Neurofibromatosis type 4 | Dan Gilbert | Daniel Craig | Notching of the ribs | NF2 |  (error) | Merlin (protein) | Alcino J. Silva | Artaxerxes I | Katie Piper | Dural ectasia | Poliosis |  (error) | Children's Tumor Foundation | Neuroendocrine tumor | Microdeletion syndrome | Cherubism | Joseph Merrick | Hypertelorism | Ulisse Aldrovandi | Acute lymphoblastic leukemia | Facial weakness | Meningioma | Expressivity (genetics) | Brain tumor | Ependymoma | Friedrich Daniel von Recklinghausen | Frank W. Crowe | Point mutation | Intracranial aneurysm | Imatinib | Conditions comorbid to autism spectrum disorders | Heterochromia iridum | Optic nerve glioma | Noonan syndrome with multiple lentigines | Spinal tumor | List of skin conditions | Chiasmal syndrome | NF |  (error) | Jar City (film) | RASopathy | Otology | Vestibular schwannoma | Achaemenid Empire | Cro-Magnon rock shelter | Sarcoma | Schwannoma | Huang Chuncai | Auditory brainstem implant | Disfigurement | Pilocytic astrocytoma | Proteus syndrome | Hamartoma | Schwannomatosis | Type 2 | Penetrance |  (error) | Macrocephaly | Birthmark | NF1 | Jaundice | List of diseases (N) | Cataract |  (error) | Crowe | Propolis |  (error) | Genodermatosis | SPRED1 | John Henry Wishart | Anaplastic astrocytoma | Fibromatosis | Moyamoya disease | Winged scapula | Macroglossia | Neuro-cardio-facial-cutaneous syndromes | Selumetinib | Synovial sarcoma | Neuroblastoma | Hemothorax | Meena Upadhyaya | Hypotonia | Under the Skin (2013 film) | Mesothelioma | Dr. Pimple Popper (TV series) | Added 20499 words: Total 0 words\n"
     ]
    }
   ],
   "source": [
    "corpus = add_corpus(\"Neurofibromatosis\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a generic symptom / medical vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 20 pages...\n",
      "Signs and symptoms | Extrapyramidal symptoms | Aura (symptom) | Anorexia (symptom) | Symptom of the Universe | Somatic symptom disorder | COVID Symptom Study | Symptom Checklist 90 | B symptoms | Palliative care | Limited symptom attack | Prodrome | Vegetative symptoms | Parkinson's disease | Drug withdrawal | Functional symptom | Dissociation (psychology) | Schizophrenia | Constitutional symptoms | Depression (mood) | Added 2062 words: Total 22561 words\n"
     ]
    }
   ],
   "source": [
    "corpus = add_corpus(\"Symptom\", 20, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 20 pages...\n",
      "Health | Health (film) | Mental health | Health care | Public health | World Health Organization | Syneos Health | Health Secretary | United States Department of Health and Human Services | Health (Health album) | Health assessment | What the Health | Health Net | Community health | United States Secretary of Health and Human Services | Reproductive health | Teladoc Health | Bausch Health | Health indicator | CVS Health |  (error) | Added 3751 words: Total 26312 words\n"
     ]
    }
   ],
   "source": [
    "corpus = add_corpus(\"Health\", 20, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 20 pages...\n",
      "Pain | Pain (disambiguation) |  (error) | Pain & Gain | To the Pain | T-Pain | The Pain | House of Pain |  (error) | Psychological pain | Knee pain | No Pain for Cakes | Pelvic pain | Abdominal pain | Chest pain | Analgesic | Low back pain | Brain Pain | Ear pain | Threshold of pain | No pain, no gain | War and Pain | Added 2597 words: Total 28909 words\n"
     ]
    }
   ],
   "source": [
    "corpus = add_corpus(\"Pain\", 20, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 10 pages...\n",
      "Emotion | Emotion classification | The Emotions | Emotion (disambiguation) |  (error) | Appeal to emotion | Fisker EMotion | Guilt (emotion) | Sweet Emotion | Passion (emotion) | Music and emotion | Added 1969 words: Total 28803 words\n"
     ]
    }
   ],
   "source": [
    "corpus = add_corpus(\"Emotion\", 10, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop if it does not contain any alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {z[0] for z in [re.findall(\"[a-z]+[a-z0-9-]*\", x) for x in corpus] if z}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28464"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing to reduce dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {lemmatize(x) for x in corpus}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spelling\n",
    "To increase the accuracy, we will apply an autocorrect model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_correct(text, corpus):\n",
    "    ''' \n",
    "    INPUT: a string object \n",
    "    RETURN: new corrected string\n",
    "    '''\n",
    "    \n",
    "    # for each word in text\n",
    "    corrected = []\n",
    "    for word in text.split():\n",
    "        if word not in corpus:\n",
    "            corrected.append(str(TextBlob(word).correct()))\n",
    "        else:\n",
    "            corrected.append(word)\n",
    "    return ' '.join(corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence treatment\n",
    "We identify symptoms per sentence. But not everyone writes a proper sentence or write one symptom per sentence. To account for this, we will create extra breaks. \n",
    "1. Break a sentence before `and`, `or`, `,`, `&`, `;`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_sentence(text):\n",
    "    '''\n",
    "    INPUT: a string object\n",
    "    RETURN: break sentence\n",
    "    '''\n",
    "    new_text = re.sub(r\"\\n\", \" \", text)\n",
    "    return re.sub(r\"(\\W*)(and|or|,|&|;)(\\W)+\", r\". \\3\", new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_full_text(text, corpus, sw = ['i', 'me', 'my', 'myself', 'we', 'our',\n",
    "                         'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your',\n",
    "                         'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she',\n",
    "                         \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them',\n",
    "                         'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\",\n",
    "                         'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have',\n",
    "                         'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but',\n",
    "                         'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n",
    "                         'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',\n",
    "                         'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "                         'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all',\n",
    "                         'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such',\n",
    "                         'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n",
    "                         's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now',\n",
    "                         'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "                         'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "                         \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\",\n",
    "                         'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
    "                         \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'felt', 'feel', 'feels'], \n",
    "                        sentence_break = True):\n",
    "    '''\n",
    "    Takes a text as an input\n",
    "    Preprocess (remove punctuations, turn lower case, lemmatize, remove stop words)\n",
    "    Return a dictionary of sentence tokens ({1: sentence}) and a nested tokens [[tokens], ...]\n",
    "    '''\n",
    "    if isinstance(text, str):\n",
    "        text = ascii_only(text.lower())\n",
    "        \n",
    "        if sentence_break:\n",
    "            text = break_sentence(text)\n",
    "        \n",
    "        text_tokens = []\n",
    "        sent_tokens = {}\n",
    "        for i, sentence in enumerate(sent_tokenize(text)): \n",
    "            cl_sentence = remove_punctuations(sentence)\n",
    "            cl_sentence = spell_correct(cl_sentence, corpus)\n",
    "            tokens = word_tokenize(cl_sentence)\n",
    "            token_set = set(lemmatize(word) for word in tokens if word not in sw)\n",
    "            \n",
    "            # only keep words in the corpus\n",
    "            token_set = corpus & token_set\n",
    "            token_set = list(token_set)\n",
    "            text_tokens.append(token_set)\n",
    "            sent_tokens[i] = sentence\n",
    "        return sent_tokens, text_tokens\n",
    "    else: \n",
    "        return 'no input'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec training\n",
    "Below is the method to create a vector space by training with word2vec. For the current version we will use the pre-trained model. But once we obtain enough text data, we can retrain the vector space with more targeted language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we will use GloVe \n",
    "# for initial run \n",
    "model = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model/word2vec.model\")\n",
    "model = KeyedVectors.load(\"model/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_sims(replace=True) # normalize if we need to retrain, remove replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model/word2vec_norm.model\")\n",
    "model = KeyedVectors.load(\"model/word2vec_norm.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a vector space. \n",
    "We'll use average vector of sentence to estimate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_vectors(text, model, corpus, sentence_break = True):\n",
    "    '''\n",
    "    INPUT\n",
    "    =====\n",
    "    text: str\n",
    "    model: Word2Vec embedding model\n",
    "    RETURN\n",
    "    =====\n",
    "    \n",
    "    '''\n",
    "    if isinstance(text, str):\n",
    "        sent_tokens, text_input = preprocess_full_text(text, corpus, sentence_break = sentence_break)\n",
    "    else:\n",
    "        print(\"Input text must be a string\")\n",
    "        return\n",
    "        \n",
    "    avg_vec = []\n",
    "    \n",
    "    for sentence in text_input:\n",
    "\n",
    "        vectors = []\n",
    "        \n",
    "        for word in sentence:\n",
    "\n",
    "            try:\n",
    "                vectors.append(model[word])\n",
    "                \n",
    "            except KeyError:\n",
    "                print(f'{word} does not exist.')\n",
    "                continue\n",
    "                \n",
    "        avg = np.average(vectors, axis = 0)\n",
    "        avg_vec.append(avg)\n",
    "        \n",
    "    return sent_tokens, avg_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_names = symptoms.name.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-Do\n",
    "Before getting the average vector, let's add a bit more information so to bias the description more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurofibroma does not exist.\n",
      "neurofibroma does not exist.\n",
      "neurofibroma does not exist.\n",
      "neurofibroma does not exist.\n",
      "mpnst does not exist.\n",
      "moyamoya does not exist.\n"
     ]
    }
   ],
   "source": [
    "_, symptom_vectors = get_avg_vectors('. '.join(symptom_names), model, corpus, \n",
    "                                  sentence_break = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_vectors = dict(zip(symptom_names, symptom_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['symptom_vectors']"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(symptom_vectors, 'symptom_vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sentence, see how close they are to target symptoms\n",
    "\n",
    "def identify_symptom(text, symptom_vectors, model, corpus, threshold = 0.5):\n",
    "    '''\n",
    "    Find the closest symptom per sentences\n",
    "    '''\n",
    "    sent_tokens, avg_vec = get_avg_vectors(text, model, corpus)\n",
    "    reference = {} # {1: {sentence: identified symptom}}\n",
    "    pred_symptoms = {}\n",
    "\n",
    "    for i, sent_vec in enumerate(avg_vec): \n",
    "        \n",
    "        # for each sentence\n",
    "        max_ = threshold\n",
    "\n",
    "        for symptom, sym_vec in symptom_vectors.items():\n",
    "            # get cosine similarity\n",
    "            similarity =  1 - cosine(sent_vec, sym_vec)\n",
    "            # find the highest similarity\n",
    "            if similarity > max_:\n",
    "\n",
    "                max_ = similarity\n",
    "                max_symptom = symptom\n",
    "        \n",
    "        \n",
    "        if max_ > threshold:\n",
    "            if max_symptom in pred_symptoms: \n",
    "                \n",
    "                # if symptom already exists, update if similarity is higher\n",
    "                if max_ > pred_symptoms[max_symptom] : \n",
    "                    pred_symptoms[max_symptom] = max_\n",
    "\n",
    "            else: \n",
    "                # add symptom if it does not exist\n",
    "                pred_symptoms[max_symptom] = max_\n",
    "                \n",
    "        reference[i] = {sent_tokens[i] : max_symptom}\n",
    "\n",
    "    if pred_symptoms: \n",
    "        return [k for k, v in sorted(pred_symptoms.items(), key = lambda item: item[1])], reference        \n",
    "    else: \n",
    "        # Flag if no symptom was detected.\n",
    "        return ['NA'], reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = get_avg_vectors(text, model, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vision changes',\n",
       " 'Numbness',\n",
       " 'Problem with movement',\n",
       " 'High blood pressure',\n",
       " 'Pain',\n",
       " 'Headaches or migraines']"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I have a high fever and headache. Things look a little strange. Pain on the left side of the body. Also felt a bit of numbness. It seems like I had hard time moving my limbs\"\n",
    "result, reference = identify_symptom(text, symptom_vectors, model, corpus)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'i have a high fever.': 'High blood pressure'},\n",
       " 1: {'headache.': 'Headaches or migraines'},\n",
       " 2: {'things look a little strange.': 'Vision changes'},\n",
       " 3: {'pain on the left side of the body.': 'Pain'},\n",
       " 4: {'also felt a bit of numbness.': 'Numbness'},\n",
       " 5: {'it seems like i had hard time moving my limbs': 'Problem with movement'}}"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_symptom_id(symptom_list, keys):\n",
    "    return [keys[keys.symptom == x].index[0] for x in symptom_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 25, 24, 21, 58]"
      ]
     },
     "execution_count": 927,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return symptom_id for this\n",
    "result_symptom_id = return_symptom_id(result, keys)\n",
    "result_symptom_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next iteration, we can present users with the select list of symptoms so they can provide a feedback as to how accurate our model is, then retrain based on their answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Testing using medical transcription data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = pd.read_csv('data/medical_transcription_samples.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4999 entries, 0 to 4998\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   description        4999 non-null   object\n",
      " 1   medical_specialty  4999 non-null   object\n",
      " 2   sample_name        4999 non-null   object\n",
      " 3   transcription      4966 non-null   object\n",
      " 4   keywords           3931 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 234.3+ KB\n"
     ]
    }
   ],
   "source": [
    "mt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing all transcriptions\n",
    "text_input = [preprocess(x) for x in mt.transcription]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# unnesting once\n",
    "text_input = list(chain(*text_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140476"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is when we train our own vector space using text_input preprocessed above\n",
    "#model = Word2Vec(sentences = text_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
