{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symptom Identifier\n",
    "This document contains exploration to build the NLP model to determine probability or similarity of how much given text indicates each symptom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import joblib\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia as wiki\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Symptoms\n",
    "Now I will write some simple nlp steps to extract symptoms from descriptions. Until we collect enough text data, we will rely on the vector space provided by Spacy  to determine how closely a description is related to the each symptoms. We'll also see if the public domain medical transcription data will create a better vector space to identify the symptoms from descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing symptoms\n",
    "symptoms = pd.read_json('data/symptoms.json', orient = 'table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "first, preprocessing steps for text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import bigrams\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text, punctuations = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~�0123456789®'):\n",
    "    ''' remove punctuations '''\n",
    "    table_ = str.maketrans('', '', punctuations)\n",
    "    return text.translate(table_)\n",
    "\n",
    "def ascii_only(text):\n",
    "    ''' remove non-ascii words '''\n",
    "    return text.encode(\"ascii\", \"ignore\").decode()\n",
    "\n",
    "def lemmatize(word):\n",
    "    ''' lemmatize text'''\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return wnl.lemmatize(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Corpus\n",
    "Initially, we will build the corpus using the relevant Wikipedia page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(pages, corpus = set()):\n",
    "    print(f\"Reading {len(pages)} pages...\")\n",
    "    for pg in pages: \n",
    "        print(pg, end = '')\n",
    "        try:\n",
    "            print(' | ', end = '')\n",
    "            soup = BeautifulSoup(wiki.page(pg).html(), 'html.parser')\n",
    "        except:\n",
    "            print(' (error) | ', end = '')\n",
    "            continue\n",
    "        text = re.sub('\\n', ' ', soup.get_text())\n",
    "        text = re.sub(\"\\[[^\\[\\]]*\\]\", \" \", text)\n",
    "        text = re.sub(\"{[^{}]*}\", \"\", text)\n",
    "        text = re.sub(\"\\.[a-z0-9-]+\", \"\", text)\n",
    "        text = re.sub(\"References  .+\", \"\", text)\n",
    "        text = re.sub(r\"([^A-Z-(])([A-Z])\", r\"\\1 \\2\", text)\n",
    "        text = re.sub(\"\\xa0[0-9]*\", \" \", text)\n",
    "        text = remove_punctuations(text, punctuations = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~�®')\n",
    "        words = set(x.lower() for x in text.split() if len(x) > 1)\n",
    "        corpus = corpus | words\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_corpus(search_term, max_pages, corpus = set()):\n",
    "    pages = wiki.search(search_term, results=max_pages)\n",
    "    current_length = len(corpus)\n",
    "    new_corpus = get_words(pages, corpus)\n",
    "    print(f\"Added {len(new_corpus) - current_length} words: Total {len(new_corpus)} words\")\n",
    "    return new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 100 pages...\n",
      "Neurofibromatosis | Neurofibromatosis type I | Neurofibromatosis type II | Café au lait spot | Legius syndrome | Gillian Anderson |  (error) | Neurofibroma | Genetic disorder | Neurofibromatosis type 3 | Watson syndrome | Malignant peripheral nerve sheath tumor | Adam Pearson (actor) | Neurofibromin 1 | Noonan syndrome | Lisch nodule | Neurofibromin | Glioblastoma | Scoliosis | Phakomatosis | Crowe sign | Neurofibromatosis type 4 | Dan Gilbert | Daniel Craig | Notching of the ribs | NF2 |  (error) | Merlin (protein) | Alcino J. Silva | Artaxerxes I | Katie Piper | Dural ectasia | Poliosis |  (error) | Children's Tumor Foundation | Neuroendocrine tumor | Microdeletion syndrome | Cherubism | Joseph Merrick | Hypertelorism | Ulisse Aldrovandi | Acute lymphoblastic leukemia | Facial weakness | Meningioma | Expressivity (genetics) | Brain tumor | Ependymoma | Friedrich Daniel von Recklinghausen | Frank W. Crowe | Point mutation | Intracranial aneurysm | Imatinib | Conditions comorbid to autism spectrum disorders | Heterochromia iridum | Optic nerve glioma | Noonan syndrome with multiple lentigines | Spinal tumor | List of skin conditions | Chiasmal syndrome | NF |  (error) | Jar City (film) | RASopathy | Otology | Vestibular schwannoma | Achaemenid Empire | Cro-Magnon rock shelter | Sarcoma | Schwannoma | Huang Chuncai | Auditory brainstem implant | Disfigurement | Pilocytic astrocytoma | Proteus syndrome | Hamartoma | Schwannomatosis | Type 2 | Penetrance |  (error) | Macrocephaly | Birthmark | NF1 | Jaundice | List of diseases (N) | Cataract |  (error) | Crowe | Propolis |  (error) | Genodermatosis | SPRED1 | John Henry Wishart | Anaplastic astrocytoma | Fibromatosis | Moyamoya disease | Winged scapula | Macroglossia | Neuro-cardio-facial-cutaneous syndromes | Selumetinib | Synovial sarcoma | Neuroblastoma | Hemothorax | Meena Upadhyaya | Hypotonia | Under the Skin (2013 film) | Mesothelioma | Dr. Pimple Popper (TV series) | Added 20499 words: Total 0 words\n"
     ]
    }
   ],
   "source": [
    "corpus = add_corpus(\"Neurofibromatosis\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a generic symptom / medical vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 20 pages...\n",
      "Signs and symptoms | Extrapyramidal symptoms | "
     ]
    }
   ],
   "source": [
    "corpus = add_corpus(\"Symptom\", 20, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 20 pages...\n",
      "Health | Health (film) | Mental health | Health care | Public health | World Health Organization | Syneos Health | Health Secretary | United States Department of Health and Human Services | Health (Health album) | Health assessment | What the Health | Health Net | Community health | United States Secretary of Health and Human Services | Reproductive health | Teladoc Health | Bausch Health | Health indicator | CVS Health |  (error) | Added 0 words: Total 22561 words\n"
     ]
    }
   ],
   "source": [
    "corpus = add_corpus(\"Health\", 20, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = add_corpus(\"Pain\", 20, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop if it does not contain any alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {z[0] for z in [re.findall(\"[a-z]+[a-z0-9-]*\", x) for x in corpus] if z}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4443"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'sad' in corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spelling\n",
    "To increase the accuracy, we will apply an autocorrect model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrect(text):\n",
    "    ''' to-do '''\n",
    "    # if the \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence treatment\n",
    "We identify symptoms per sentence. But not everyone writes a proper sentence or write one symptom per sentence. To account for this, we will test preprocessing sentences using below measures. \n",
    "1. Break a sentence before 'and', ',', '&',\n",
    "2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' to-do '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, sw = ['i', 'me', 'my', 'myself', 'we', 'our',\n",
    "                         'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your',\n",
    "                         'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she',\n",
    "                         \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them',\n",
    "                         'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\",\n",
    "                         'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have',\n",
    "                         'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but',\n",
    "                         'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n",
    "                         'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',\n",
    "                         'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "                         'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all',\n",
    "                         'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such',\n",
    "                         'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n",
    "                         's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now',\n",
    "                         'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "                         'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "                         \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\",\n",
    "                         'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
    "                         \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'felt', 'feel', 'feels']):\n",
    "    '''\n",
    "    Takes a text as an input\n",
    "    Preprocess (remove punctuations, turn lower case, lemmatize, remove stop words)\n",
    "    '''\n",
    "    if isinstance(sentence, str):\n",
    "        sentence = remove_punctuations(sentence)\n",
    "            tokens = word_tokenize(sentence)\n",
    "            text_tokens.append([lemmatize(word) for word in tokens if word not in sw])\n",
    "        return text_tokens\n",
    "    else: \n",
    "        return 'no input'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_full_text(text):\n",
    "    '''\n",
    "    Takes a text as an input\n",
    "    Preprocess (remove punctuations, turn lower case, lemmatize, remove stop words)\n",
    "    Return a nested array with a tokens per sentence\n",
    "    '''\n",
    "    if isinstance(text, str):\n",
    "        text = ascii_only(text.lower())\n",
    "\n",
    "        text_tokens = []\n",
    "        for sentence in sent_tokenize(text): \n",
    "            sentence = remove_punctuations(sentence)\n",
    "            tokens = word_tokenize(sentence)\n",
    "            text_tokens.append([lemmatize(word) for word in tokens if word not in sw])\n",
    "        return text_tokens\n",
    "    else: \n",
    "        return 'no input'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec training\n",
    "Below is the method to create a vector space by training with word2vec. But for the prototype we will use the pretrained model. But once we obtain enough text data, we can retrain the vector space with more targeted language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we will use GloVe \n",
    "# for initial run \n",
    "model = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"model/word2vec.model\")\n",
    "model = Word2Vec.load(\"model/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_sims(replace=True) # normalize if we need to retrain, remove replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model/word2vec_norm.model\")\n",
    "#model = Word2Vec.load(\"model/word2vec_norm.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a vector space. \n",
    "For now, we'll use average vector of sentence to estimate. But in the future, we will turn this into doc2vec and check the sentence similarity instead of word similarity to increase accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_vectors(text, model):\n",
    "    if isinstance(text, str):\n",
    "        text_input = preprocess(text)\n",
    "    else: \n",
    "        text_input = text.copy()\n",
    "    avg_vec = []\n",
    "    for sentence in text_input:\n",
    "        vectors = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vectors.append(model[word])\n",
    "            except KeyError:\n",
    "                print(f'{word} not exists')\n",
    "                pass\n",
    "        avg = np.average(vectors, axis = 0)\n",
    "        avg_vec.append(avg)\n",
    "    return avg_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptoms = keys.symptom.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurofibroma not exists\n",
      "neurofibroma not exists\n",
      "neurofibroma not exists\n",
      "neurofibroma not exists\n",
      "mpnst not exists\n",
      "moyamoya not exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py:390: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "symptom_vectors = get_avg_vectors('. '.join(symptoms), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_vectors = dict(zip(symptoms, symptom_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['symptom_vectors.pkl']"
      ]
     },
     "execution_count": 776,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(symptom_vectors, 'symptom_vectors.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sentence, see how close they are to target symptoms\n",
    "\n",
    "def identify_symptom(text, symptom_vectors, model, threshold = 0.5):\n",
    "    '''\n",
    "    Find the closest symptom per sentences\n",
    "    '''\n",
    "    avg_vec = get_avg_vectors(text, model)\n",
    "    pred_symptoms = {}\n",
    "\n",
    "    for sent_vec in avg_vec: \n",
    "        \n",
    "        # for each sentence\n",
    "        max_ = threshold\n",
    "\n",
    "        for symptom, sym_vec in symptom_vectors.items():\n",
    "            similarity =  1 - cosine(sent_vec, sym_vec)\n",
    "            if similarity > max_:\n",
    "\n",
    "                max_ = similarity\n",
    "                max_symptom = symptom\n",
    "        if max_ > threshold:\n",
    "            try: \n",
    "                # if symptom already exists, update if similarity is higher\n",
    "                if max_ > pred_symptoms[max_symptom] : \n",
    "                    pred_symptoms[max_symptom] = max_\n",
    "\n",
    "            except: \n",
    "                # add symptom if it does not exist\n",
    "                pred_symptoms[max_symptom] = max_\n",
    "                \n",
    "    ''' TO-DO : Flag if no symptom was detected. '''\n",
    "    \n",
    "    return [k for k, v in sorted(pred_symptoms.items(), key = lambda item: item[1])]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I have a high fever and headache. Things look a little strange. Pain on the left side of the body. Also felt a bit of numbness. It seems like I had hard time moving my limbs\"\n",
    "result = identify_symptom(text, symptom_vectors, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 3)"
      ]
     },
     "execution_count": 936,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_symptom_id(symptom_list, keys):\n",
    "    return [keys[keys.symptom == x].index[0] for x in symptom_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 25, 24, 21, 58]"
      ]
     },
     "execution_count": 927,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return symptom_id for this\n",
    "result_symptom_id = return_symptom_id(result, keys)\n",
    "result_symptom_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next iteration, we can present users with the select list of symptoms so they can provide a feedback as to how accurate our model is, then retrain based on their answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Testing using medical transcription data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = pd.read_csv('data/medical_transcription_samples.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4999 entries, 0 to 4998\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   description        4999 non-null   object\n",
      " 1   medical_specialty  4999 non-null   object\n",
      " 2   sample_name        4999 non-null   object\n",
      " 3   transcription      4966 non-null   object\n",
      " 4   keywords           3931 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 234.3+ KB\n"
     ]
    }
   ],
   "source": [
    "mt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing all transcriptions\n",
    "text_input = [preprocess(x) for x in mt.transcription]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# unnesting once\n",
    "text_input = list(chain(*text_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140476"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is when we train our own vector space using text_input preprocessed above\n",
    "#model = Word2Vec(sentences = text_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
