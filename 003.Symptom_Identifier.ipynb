{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symptom Identifier\n",
    "This document contains exploration to build the NLP model to determine probability or similarity of how much given text indicates each symptom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import joblib\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia as wiki\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import bigrams\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Symptoms\n",
    "Now I will write some simple nlp steps to extract symptoms from descriptions. Until we collect enough text data, we will rely on the vector space provided by Spacy  to determine how closely a description is related to the each symptoms. We'll also see if the public domain medical transcription data will create a better vector space to identify the symptoms from descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing symptoms\n",
    "symptoms = pd.read_json('data/symptoms.json', orient = 'table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "first, preprocessing steps for text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text, punctuations = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~�0123456789®'):\n",
    "    ''' remove punctuations '''\n",
    "    table_ = str.maketrans(punctuations, ' '*len(punctuations))\n",
    "    return text.translate(table_)\n",
    "\n",
    "def ascii_only(text):\n",
    "    ''' remove non-ascii words '''\n",
    "    return text.encode(\"ascii\", \"ignore\").decode()\n",
    "\n",
    "def lemmatize(word):\n",
    "    ''' lemmatize text'''\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return wnl.lemmatize(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Corpus\n",
    "Initially, we will build the corpus using the relevant Wikipedia page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(pages, corpus = set()):\n",
    "    print(f\"Reading {len(pages)} pages...\")\n",
    "    for pg in pages: \n",
    "        print(pg, end = '')\n",
    "        try:\n",
    "            print(' | ', end = '')\n",
    "            soup = BeautifulSoup(wiki.page(pg).html(), 'html.parser')\n",
    "        except:\n",
    "            print(' (error) | ', end = '')\n",
    "            continue\n",
    "        text = re.sub('\\n', ' ', soup.get_text())\n",
    "        text = re.sub(\"\\[[^\\[\\]]*\\]\", \" \", text)\n",
    "        text = re.sub(\"{[^{}]*}\", \"\", text)\n",
    "        text = re.sub(\"\\.[a-z0-9-]+\", \"\", text)\n",
    "        text = re.sub(\"References  .+\", \"\", text)\n",
    "        text = re.sub(r\"([^A-Z-(])([A-Z])\", r\"\\1 \\2\", text)\n",
    "        text = re.sub(\"\\xa0[0-9]*\", \" \", text)\n",
    "        text = remove_punctuations(text, punctuations = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~�®')\n",
    "        words = set(x.lower() for x in text.split() if len(x) > 1)\n",
    "        corpus = corpus | words\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_corpus(search_term, max_pages, corpus = set()):\n",
    "    pages = wiki.search(search_term, results=max_pages)\n",
    "    current_length = len(corpus)\n",
    "    new_corpus = get_words(pages, corpus)\n",
    "    print(f\"Added {len(new_corpus) - current_length} words: Total {len(new_corpus)} words\")\n",
    "    return new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop here. skip to load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 100 pages...\n",
      "Neurofibromatosis | Neurofibromatosis type I | Neurofibromatosis type II |  (error) | Café au lait spot | Gillian Anderson |  (error) | Legius syndrome | Neurofibroma | Noonan syndrome | Neurofibromatosis type 3 | Genetic disorder | Adam Pearson (actor) | Malignant peripheral nerve sheath tumor | Lisch nodule | Neurofibromin 1 | Phakomatosis | Watson syndrome | Neurofibromatosis type 4 | Neurofibromin | Glioblastoma | Crowe sign | Dan Gilbert | Daniel Craig | NF2 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /opt/anaconda3/lib/python3.7/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (error) | Notching of the ribs | Katie Piper | Microdeletion syndrome | Scoliosis | Poliosis |  (error) | Dural ectasia | Alcino J. Silva | Hypertelorism | Artaxerxes I | Cherubism | Neuroendocrine tumor | Facial weakness | Joseph Merrick | Merlin (protein) | Children's Tumor Foundation | Acute lymphoblastic leukemia | Brain tumor | Ulisse Aldrovandi | Expressivity (genetics) | List of skin conditions | Point mutation | Optic nerve glioma | Friedrich Daniel von Recklinghausen | Conditions comorbid to autism spectrum disorders | Imatinib | Noonan syndrome with multiple lentigines | Spinal tumor | Heterochromia iridum | RASopathy | Ependymoma | Chiasmal syndrome | Frank W. Crowe | Vestibular schwannoma | NF |  (error) | Otology | Proteus syndrome | Achaemenid Empire | Jar City (film) | Auditory brainstem implant | Sarcoma | Cro-Magnon rock shelter | Meningioma | Huang Chuncai | Birthmark | Penetrance |  (error) | Schwannoma | Hypotonia | Type 2 | Crowe | Schwannomatosis | NF1 | Pilocytic astrocytoma | Hamartoma | Disfigurement | Macrocephaly | List of diseases (N) | Fibromatosis | Genodermatosis | John Henry Wishart | Intracranial aneurysm | Cataract |  (error) | Moyamoya disease | Propolis |  (error) | Macroglossia | Anaplastic astrocytoma | Dr. Pimple Popper (TV series) | Selumetinib | Winged scapula | Mesothelioma | Neuro-cardio-facial-cutaneous syndromes | Hemothorax | Neuroblastoma | SPRED1 | Meena Upadhyaya | Meningioangiomatosis | List of medical roots, suffixes and prefixes | Soft-tissue sarcoma | Added 22027 words: Total 22027 words\n"
     ]
    }
   ],
   "source": [
    "corpus = add_corpus(\"Neurofibromatosis\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a generic symptom / medical vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 20 pages...\n",
      "Signs and symptoms | Extrapyramidal symptoms | Aura (symptom) | Symptom of the Universe | Somatic symptom disorder | Anorexia (symptom) | Palliative care | Symptom Checklist 90 | B symptoms | COVID Symptom Study | Functional symptom | Drug withdrawal | Vegetative symptoms | Dissociation (psychology) | Limited symptom attack | Schizophrenia | Prodrome | Constitutional symptoms | Parkinson's disease | Depression (mood) | Added 2026 words: Total 22267 words\n"
     ]
    }
   ],
   "source": [
    "corpus = add_corpus(\"Symptom\", 20, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 20 pages...\n",
      "Health | Health (film) | Mental health | Health care | Public health | World Health Organization | Syneos Health | Health insurance | Environmental health | United States Department of Health and Human Services | Health indicator | Reproductive health | Health assessment | Health Secretary | Health professional | Health (Health album) | Bausch Health | CVS Health |  (error) | Health benefit |  (error) | Health board |  (error) | Added 3434 words: Total 25701 words\n"
     ]
    }
   ],
   "source": [
    "corpus = add_corpus(\"Health\", 20, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 20 pages...\n",
      "Pain | Pain (disambiguation) |  (error) | Pain & Gain | T-Pain | To the Pain | Pain management | Power and Pain | Psychological pain | Pain stimulus | Knee pain | Neuropathic pain | Low back pain | Pelvic pain | Radicular pain | Analgesic | Bedabrata Pain | No Pain for Cakes | Abdominal pain | The Pain | Chest pain | Added 2730 words: Total 28431 words\n"
     ]
    }
   ],
   "source": [
    "corpus = add_corpus(\"Pain\", 20, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 10 pages...\n",
      "Emotion | Emotion classification | The Emotions | Emotion (disambiguation) |  (error) | Appeal to emotion | Emotion recognition | Art and emotion | Guilt (emotion) | Fisker EMotion | Passion (emotion) | Added 1457 words: Total 29888 words\n"
     ]
    }
   ],
   "source": [
    "corpus = add_corpus(\"Emotion\", 10, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop if it does not contain any alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {z[0] for z in [re.findall(\"[a-z]+[a-z0-9-]*\", x) for x in corpus] if z}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27644"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing to reduce dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {lemmatize(x) for x in corpus}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/corpus']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joblib.dump(corpus, 'data/corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = joblib.load('data/corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spelling\n",
    "To increase the accuracy, we will apply an autocorrect model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_correct(text, corpus):\n",
    "    ''' \n",
    "    INPUT: a string object \n",
    "    RETURN: new corrected string\n",
    "    '''\n",
    "    \n",
    "    # for each word in text\n",
    "    corrected = []\n",
    "    for word in text.split():\n",
    "        if word not in corpus:\n",
    "            corrected.append(str(TextBlob(word).correct()))\n",
    "        else:\n",
    "            corrected.append(word)\n",
    "    return ' '.join(corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence treatment\n",
    "We identify symptoms per sentence. But not everyone writes a proper sentence or write one symptom per sentence. To account for this, we will create extra breaks. \n",
    "1. Break a sentence before `and`, `or`, `,`, `&`, `;`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_sentence(text):\n",
    "    '''\n",
    "    INPUT: a string object\n",
    "    RETURN: break sentence\n",
    "    '''\n",
    "    new_text = re.sub(r\"\\n\", \". \", text)\n",
    "    return re.sub(r\"(\\W*)(and|or|,|&|;)(\\W)+\", r\". \\3\", new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_full_text(text, corpus, sw = ['i', 'me', 'my', 'myself', 'we', 'our',\n",
    "                         'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your',\n",
    "                         'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she',\n",
    "                         \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them',\n",
    "                         'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\",\n",
    "                         'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have',\n",
    "                         'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but',\n",
    "                         'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n",
    "                         'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',\n",
    "                         'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "                         'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all',\n",
    "                         'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such',\n",
    "                         'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n",
    "                         's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now',\n",
    "                         'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "                         'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "                         \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\",\n",
    "                         'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
    "                         \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'felt', 'feel', 'feels'], \n",
    "                        sentence_break = True):\n",
    "    '''\n",
    "    Takes a text as an input\n",
    "    Preprocess (remove punctuations, turn lower case, lemmatize, remove stop words)\n",
    "    Return a dictionary of sentence tokens ({1: sentence}) and a nested tokens [[tokens], ...]\n",
    "    '''\n",
    "    if isinstance(text, str):\n",
    "        text = ascii_only(text.lower())\n",
    "        \n",
    "        if sentence_break:\n",
    "            text = break_sentence(text)\n",
    "        \n",
    "        text_tokens = []\n",
    "        sent_tokens = {}\n",
    "        for i, sentence in enumerate(sent_tokenize(text)): \n",
    "            cl_sentence = remove_punctuations(sentence)\n",
    "            cl_sentence = spell_correct(cl_sentence, corpus)\n",
    "            tokens = word_tokenize(cl_sentence)\n",
    "            token_set = set(lemmatize(word) for word in tokens if word not in sw)\n",
    "            \n",
    "            # only keep words in the corpus\n",
    "            token_set = corpus & token_set\n",
    "            token_set = list(token_set)\n",
    "            text_tokens.append(token_set)\n",
    "            sent_tokens[i] = sentence\n",
    "        return sent_tokens, text_tokens\n",
    "    else: \n",
    "        return 'no input'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec training\n",
    "Below is the method to create a vector space by training with word2vec. For the current version we will use the pre-trained model. But once we obtain enough text data, we can retrain the vector space with more targeted language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we will use GloVe \n",
    "# for initial run \n",
    "# model = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"model/word2vec.model\")\n",
    "# model = KeyedVectors.load(\"model/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.init_sims(replace=True) # normalize if we need to retrain, remove replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"model/word2vec_norm.model\")\n",
    "model = KeyedVectors.load(\"model/word2vec_norm.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a vector space. \n",
    "We'll use average vector of sentence to estimate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_vectors(text, model, corpus, sentence_break = True):\n",
    "    '''\n",
    "    INPUT\n",
    "    =====\n",
    "    text: str\n",
    "    model: Word2Vec embedding model\n",
    "    RETURN\n",
    "    =====\n",
    "    \n",
    "    '''\n",
    "    if isinstance(text, str):\n",
    "        sent_tokens, text_input = preprocess_full_text(text, corpus, sentence_break = sentence_break)\n",
    "    else:\n",
    "        print(\"Input text must be a string\")\n",
    "        return\n",
    "        \n",
    "    avg_vec = []\n",
    "    \n",
    "    for sentence in text_input:\n",
    "\n",
    "        vectors = []\n",
    "        \n",
    "        for word in sentence:\n",
    "\n",
    "            try:\n",
    "                vectors.append(model[word])\n",
    "                \n",
    "            except KeyError:\n",
    "                print(f'{word} does not exist.')\n",
    "                continue\n",
    "                \n",
    "        avg = np.average(vectors, axis = 0)\n",
    "        avg_vec.append(avg)\n",
    "        \n",
    "    return sent_tokens, avg_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only symptoms (adding a bit more details to help with vector averaging)\n",
    "symptoms = {'Spots': 'Spots, specks on skin', \n",
    "            'Itching': 'Itching',\n",
    "            'Cognitive Difficulties': 'Thinking Difficulties', \n",
    "            'Vision Changes': 'Vision changes', \n",
    "            'Fractures' : 'Broken bone, fractures', \n",
    "            'Pain': 'pain',\n",
    "            'Bowel or bladder control problems': 'bowel or bladder control problem', \n",
    "            'Breathing problems': 'breathing problems',\n",
    "            'Problem with movement': 'problem with movement, balance', \n",
    "            'Numbness': 'loss of sensation, numbness, tingling, pins-and-needles, numb', \n",
    "            'Learning difficulties': 'learning disabilities',\n",
    "            'Attention issues': 'attention issues, adhd, focusing', \n",
    "            'Nosebleed': 'nosebleed, bleeding nose', \n",
    "            'Heart Problem': 'heart problem, pounding in the chest',\n",
    "            'High blood pressure': 'high blood pressure',\n",
    "            'Chewing or swallowing problems': 'difficulty chewing or swallowing, dysphagia', \n",
    "            'Constipation': 'constipation, pooping', \n",
    "            'Poor weight gain': 'poor weight gain',\n",
    "            'Gastroesophageal reflux': 'digestive disorder, gastroesophageal reflux disease, gerd, acidic stomach juices, acid refulx', \n",
    "            'Anxiety': 'anxiety, fear, worry, panic, attack, agoraphobia, phobias', \n",
    "            'Arthritis': 'arthritis, swelling and tenderness of joints, stiff limbs, pain', \n",
    "            'Depression': 'depression',\n",
    "            'Difficulties with social interactions': 'difficulties with social interactions', \n",
    "            'Fatigue': 'fatigue',\n",
    "            'Headaches or migraines': 'headache or migraines, face pain', \n",
    "            'Joint pain': 'joint pain, hip, shoulders, elbows, knees, wrists, ankles', \n",
    "            'Loose joints': 'loose joints, hypermobility, range of motion, hip, shoulders, elbows, knees, wrists, ankles', \n",
    "            'Muscle coordination issues': 'muscle coordination issues, ataxia',\n",
    "            'Other mental health problems': 'other mental health problems, ptsd, psychosis', \n",
    "            'Seizures or epilepsy': 'seizure of epilepsy', \n",
    "            'Sleep disturbances': 'sleep disturbances, insomnia', \n",
    "            'Fever': 'fever'}     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem to solve\n",
    "It needs to categorize other symptoms not-relevant to NF.  \n",
    "(e.g. \"high fever\" should not be categorized as \"high blood pressure\")  \n",
    "It also needs to weigh heavier on the word combo. \n",
    "(e.g. \"my head hurts\" should be closer to 'headache' than 'large head size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop here. skip to load the symptom_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, symptom_vectors = get_avg_vectors('. '.join(symptoms.values()), model, corpus, \n",
    "                                  sentence_break = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(symptom_vectors) == len(symptoms.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_vectors = dict(zip(symptoms.keys(), symptom_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['symptom_vectors']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(symptom_vectors, 'symptom_vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_vectors = joblib.load('symptom_vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sentence, see how close they are to target symptoms\n",
    "### To-Do: update to include all symptoms with high similarity, order them\n",
    "### Must return - output list, second likely batch, and the reference (sentence to symptom match)\n",
    "\n",
    "def identify_symptom(text, symptom_vectors, model, corpus, threshold = 0.5):\n",
    "    '''\n",
    "    Find the closest symptom per sentences\n",
    "    '''\n",
    "    sent_tokens, avg_vec = get_avg_vectors(text, model, corpus)\n",
    "    reference = {} # {1: {sentence: [(symptom, similarity), ...]}\n",
    "    #pred_symptoms = {} \n",
    "\n",
    "    for i, sent_vec in enumerate(avg_vec): \n",
    "        \n",
    "        similarity_list = []\n",
    "        \n",
    "        for symp, sym_vec in symptom_vectors.items():\n",
    "            # get similarity between symptom and sentence\n",
    "            similarity = 1 - cosine(sent_vec, sym_vec)\n",
    "            if similarity > threshold: \n",
    "                similarity_list.append((symp, round(similarity, 3)))\n",
    "            \n",
    "        reference[i] = {sent_tokens[i] : [x for x in sorted(similarity_list, \n",
    "                                                            key = lambda v: v[1], \n",
    "                                                            reverse = True)]}\n",
    "    # get the highest similarity list\n",
    "    vals = [x[0] for x in [list(x.values()) for x in reference.values()] if x[0]]\n",
    "\n",
    "    # check if it predicted ANY symptom\n",
    "    if vals: \n",
    "        first_options = set(x[0][0] for x in vals)\n",
    "        second_options = set(x[1][0] for x in vals if len(x) > 1)\n",
    "        second = second_options - first_options\n",
    "        return first_options, len(second) > 0 and second or None, reference\n",
    "        \n",
    "    else: \n",
    "        print('no symptom was detected')\n",
    "        return ['No symptom detected'], None, reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "I feel fine\n",
    "I have a high fever and headache\n",
    "My head hurts\n",
    "Things look a little strange\n",
    "Breathing was harder than usual\n",
    "Pain on the left side of the body. Also felt a bit of numbness\n",
    "It seems like I had hard time moving my limbs\n",
    "'''\n",
    "first, second, reference = identify_symptom(text, symptom_vectors, model, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Breathing problems',\n",
       "  'Fever',\n",
       "  'Headaches or migraines',\n",
       "  'Numbness',\n",
       "  'Pain',\n",
       "  'Problem with movement',\n",
       "  'Vision Changes'},\n",
       " {'Arthritis',\n",
       "  'Cognitive Difficulties',\n",
       "  'Heart Problem',\n",
       "  'High blood pressure'})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first, second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'.': []},\n",
       " 1: {'i feel fine.': []},\n",
       " 2: {'i have a high fever.': [('Fever', 0.783),\n",
       "   ('High blood pressure', 0.656)]},\n",
       " 3: {'headache.': [('Headaches or migraines', 0.814),\n",
       "   ('Arthritis', 0.545),\n",
       "   ('Pain', 0.511)]},\n",
       " 4: {'my head hurts.': []},\n",
       " 5: {'things look a little strange.': [('Vision Changes', 0.54),\n",
       "   ('Cognitive Difficulties', 0.531)]},\n",
       " 6: {'breathing was harder than usual.': [('Breathing problems', 0.709),\n",
       "   ('Cognitive Difficulties', 0.584),\n",
       "   ('Poor weight gain', 0.505)]},\n",
       " 7: {'pain on the left side of the body.': [('Pain', 0.659),\n",
       "   ('Heart Problem', 0.656),\n",
       "   ('Joint pain', 0.606),\n",
       "   ('Arthritis', 0.584),\n",
       "   ('High blood pressure', 0.576),\n",
       "   ('Headaches or migraines', 0.565),\n",
       "   ('Loose joints', 0.552),\n",
       "   ('Spots', 0.539),\n",
       "   ('Problem with movement', 0.537),\n",
       "   ('Fractures', 0.508),\n",
       "   ('Poor weight gain', 0.508),\n",
       "   ('Nosebleed', 0.507)]},\n",
       " 8: {'also felt a bit of numbness.': [('Numbness', 0.585),\n",
       "   ('Pain', 0.561),\n",
       "   ('Arthritis', 0.558),\n",
       "   ('Breathing problems', 0.514),\n",
       "   ('Headaches or migraines', 0.514)]},\n",
       " 9: {'it seems like i had hard time moving my limbs.': [('Problem with movement',\n",
       "    0.642),\n",
       "   ('Cognitive Difficulties', 0.61),\n",
       "   ('Vision Changes', 0.572),\n",
       "   ('Heart Problem', 0.566),\n",
       "   ('Breathing problems', 0.529),\n",
       "   ('High blood pressure', 0.529),\n",
       "   ('Poor weight gain', 0.528)]}}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return results as index\n",
    "\n",
    "Pending -- need to decide how to (or whether to) store all similarities or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_symptom_id(symptom_list, keys):\n",
    "    return [keys[keys.symptom == x].index[0] for x in symptom_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 25, 24, 21, 58]"
      ]
     },
     "execution_count": 927,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return symptom_id for this\n",
    "result_symptom_id = return_symptom_id(result, keys)\n",
    "result_symptom_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
