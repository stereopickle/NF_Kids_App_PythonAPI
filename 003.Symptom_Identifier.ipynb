{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symptom Identifier\n",
    "This document contains exploration to build the NLP model to determine probability or similarity of how much given text indicates each symptom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import joblib\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia as wiki\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import bigrams\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Symptoms\n",
    "Now I will write some simple nlp steps to extract symptoms from descriptions. Until we collect enough text data, we will rely on the vector space provided by Spacy  to determine how closely a description is related to the each symptoms. We'll also see if the public domain medical transcription data will create a better vector space to identify the symptoms from descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing symptoms\n",
    "symptoms = pd.read_json('data/symptoms.json', orient = 'table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "first, preprocessing steps for text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text, punctuations = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~�0123456789®'):\n",
    "    ''' remove punctuations '''\n",
    "    table_ = str.maketrans(punctuations, ' '*len(punctuations))\n",
    "    return text.translate(table_)\n",
    "\n",
    "def ascii_only(text):\n",
    "    ''' remove non-ascii words '''\n",
    "    return text.encode(\"ascii\", \"ignore\").decode()\n",
    "\n",
    "def lemmatize(word):\n",
    "    ''' lemmatize text'''\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return wnl.lemmatize(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Corpus\n",
    "Initially, we will build the corpus using the relevant Wikipedia page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(pages, corpus = set()):\n",
    "    print(f\"Reading {len(pages)} pages...\")\n",
    "    for pg in pages: \n",
    "        print(pg, end = '')\n",
    "        try:\n",
    "            print(' | ', end = '')\n",
    "            soup = BeautifulSoup(wiki.page(pg).html(), 'html.parser')\n",
    "        except:\n",
    "            print(' (error) | ', end = '')\n",
    "            continue\n",
    "        text = re.sub('\\n', ' ', soup.get_text())\n",
    "        text = re.sub(\"\\[[^\\[\\]]*\\]\", \" \", text)\n",
    "        text = re.sub(\"{[^{}]*}\", \"\", text)\n",
    "        text = re.sub(\"\\.[a-z0-9-]+\", \"\", text)\n",
    "        text = re.sub(\"References  .+\", \"\", text)\n",
    "        text = re.sub(r\"([^A-Z-(])([A-Z])\", r\"\\1 \\2\", text)\n",
    "        text = re.sub(\"\\xa0[0-9]*\", \" \", text)\n",
    "        text = remove_punctuations(text, punctuations = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~�®')\n",
    "        words = set(x.lower() for x in text.split() if len(x) > 1)\n",
    "        corpus = corpus | words\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_corpus(search_term, max_pages, corpus = set()):\n",
    "    pages = wiki.search(search_term, results=max_pages)\n",
    "    current_length = len(corpus)\n",
    "    new_corpus = get_words(pages, corpus)\n",
    "    print(f\"Added {len(new_corpus) - current_length} words: Total {len(new_corpus)} words\")\n",
    "    return new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 100 pages...\n",
      "Neurofibromatosis | Neurofibromatosis type I | Neurofibromatosis type II | Café au lait spot | Gillian Anderson |  (error) | Legius syndrome | Neurofibroma | Genetic disorder | Noonan syndrome | Adam Pearson (actor) | Malignant peripheral nerve sheath tumor | Watson syndrome | Neurofibromatosis type 3 | Lisch nodule | Neurofibromin 1 | Glioblastoma | Phakomatosis | Scoliosis | Neurofibromin | Crowe sign | Neurofibromatosis type 4 | Daniel Craig | Notching of the ribs | Dan Gilbert | NF2 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /opt/anaconda3/lib/python3.7/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (error) | Alcino J. Silva | Microdeletion syndrome | Katie Piper | Dural ectasia | Poliosis |  (error) | Hypertelorism | Artaxerxes I | Cherubism | Neuroendocrine tumor | Facial weakness | Children's Tumor Foundation | Joseph Merrick | Merlin (protein) | Acute lymphoblastic leukemia | Ulisse Aldrovandi | Brain tumor | Meningioma | Expressivity (genetics) | Ependymoma | Conditions comorbid to autism spectrum disorders | Intracranial aneurysm | List of skin conditions | Point mutation | Friedrich Daniel von Recklinghausen | Imatinib | Optic nerve glioma | Heterochromia iridum | Noonan syndrome with multiple lentigines | RASopathy | Spinal tumor | Schwannomatosis | Chiasmal syndrome | NF |  (error) | Jar City (film) | Otology | Achaemenid Empire | Proteus syndrome | Huang Chuncai | Vestibular schwannoma | Sarcoma | Cro-Magnon rock shelter | Auditory brainstem implant | Penetrance |  (error) | Schwannoma | Hypotonia | Pilocytic astrocytoma | Hamartoma | Disfigurement | Type 2 | Macrocephaly | Birthmark | NF1 | Fibromatosis | Jaundice | List of diseases (N) | Frank W. Crowe | Cataract |  (error) | Crowe | Propolis |  (error) | Genodermatosis | Macroglossia | John Henry Wishart | Anaplastic astrocytoma | Neuro-cardio-facial-cutaneous syndromes | Winged scapula | Moyamoya disease | Selumetinib | Synovial sarcoma | SPRED1 | Neuroblastoma | Hemothorax | Under the Skin (2013 film) | Meena Upadhyaya | Mesothelioma | Dr. Pimple Popper (TV series) | Added 20241 words: Total 20241 words\n"
     ]
    }
   ],
   "source": [
    "corpus = add_corpus(\"Neurofibromatosis\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a generic symptom / medical vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 20 pages...\n",
      "Signs and symptoms | Extrapyramidal symptoms | Aura (symptom) | Symptom of the Universe | Somatic symptom disorder | Anorexia (symptom) | Palliative care | Symptom Checklist 90 | B symptoms | COVID Symptom Study | Functional symptom | Drug withdrawal | Vegetative symptoms | Dissociation (psychology) | Limited symptom attack | Schizophrenia | Prodrome | Constitutional symptoms | Parkinson's disease | Depression (mood) | Added 2026 words: Total 22267 words\n"
     ]
    }
   ],
   "source": [
    "corpus = add_corpus(\"Symptom\", 20, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 20 pages...\n",
      "Health | Health (film) | Mental health | Health care | Public health | World Health Organization | "
     ]
    }
   ],
   "source": [
    "corpus = add_corpus(\"Health\", 20, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = add_corpus(\"Pain\", 20, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = add_corpus(\"Emotion\", 10, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop if it does not contain any alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {z[0] for z in [re.findall(\"[a-z]+[a-z0-9-]*\", x) for x in corpus] if z}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing to reduce dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {lemmatize(x) for x in corpus}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(corpus, 'data/corpus')\n",
    "#corpus = joblib.load('data/corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spelling\n",
    "To increase the accuracy, we will apply an autocorrect model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_correct(text, corpus):\n",
    "    ''' \n",
    "    INPUT: a string object \n",
    "    RETURN: new corrected string\n",
    "    '''\n",
    "    \n",
    "    # for each word in text\n",
    "    corrected = []\n",
    "    for word in text.split():\n",
    "        if word not in corpus:\n",
    "            corrected.append(str(TextBlob(word).correct()))\n",
    "        else:\n",
    "            corrected.append(word)\n",
    "    return ' '.join(corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence treatment\n",
    "We identify symptoms per sentence. But not everyone writes a proper sentence or write one symptom per sentence. To account for this, we will create extra breaks. \n",
    "1. Break a sentence before `and`, `or`, `,`, `&`, `;`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_sentence(text):\n",
    "    '''\n",
    "    INPUT: a string object\n",
    "    RETURN: break sentence\n",
    "    '''\n",
    "    new_text = re.sub(r\"\\n\", \" \", text)\n",
    "    return re.sub(r\"(\\W*)(and|or|,|&|;)(\\W)+\", r\". \\3\", new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_full_text(text, corpus, sw = ['i', 'me', 'my', 'myself', 'we', 'our',\n",
    "                         'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your',\n",
    "                         'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she',\n",
    "                         \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them',\n",
    "                         'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\",\n",
    "                         'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have',\n",
    "                         'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but',\n",
    "                         'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',\n",
    "                         'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',\n",
    "                         'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "                         'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all',\n",
    "                         'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such',\n",
    "                         'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n",
    "                         's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now',\n",
    "                         'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
    "                         'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
    "                         \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\",\n",
    "                         'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
    "                         \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'felt', 'feel', 'feels'], \n",
    "                        sentence_break = True):\n",
    "    '''\n",
    "    Takes a text as an input\n",
    "    Preprocess (remove punctuations, turn lower case, lemmatize, remove stop words)\n",
    "    Return a dictionary of sentence tokens ({1: sentence}) and a nested tokens [[tokens], ...]\n",
    "    '''\n",
    "    if isinstance(text, str):\n",
    "        text = ascii_only(text.lower())\n",
    "        \n",
    "        if sentence_break:\n",
    "            text = break_sentence(text)\n",
    "        \n",
    "        text_tokens = []\n",
    "        sent_tokens = {}\n",
    "        for i, sentence in enumerate(sent_tokenize(text)): \n",
    "            cl_sentence = remove_punctuations(sentence)\n",
    "            cl_sentence = spell_correct(cl_sentence, corpus)\n",
    "            tokens = word_tokenize(cl_sentence)\n",
    "            token_set = set(lemmatize(word) for word in tokens if word not in sw)\n",
    "            \n",
    "            # only keep words in the corpus\n",
    "            token_set = corpus & token_set\n",
    "            token_set = list(token_set)\n",
    "            text_tokens.append(token_set)\n",
    "            sent_tokens[i] = sentence\n",
    "        return sent_tokens, text_tokens\n",
    "    else: \n",
    "        return 'no input'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec training\n",
    "Below is the method to create a vector space by training with word2vec. For the current version we will use the pre-trained model. But once we obtain enough text data, we can retrain the vector space with more targeted language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we will use GloVe \n",
    "# for initial run \n",
    "model = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model/word2vec.model\")\n",
    "model = KeyedVectors.load(\"model/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_sims(replace=True) # normalize if we need to retrain, remove replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model/word2vec_norm.model\")\n",
    "model = KeyedVectors.load(\"model/word2vec_norm.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a vector space. \n",
    "We'll use average vector of sentence to estimate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_vectors(text, model, corpus, sentence_break = True):\n",
    "    '''\n",
    "    INPUT\n",
    "    =====\n",
    "    text: str\n",
    "    model: Word2Vec embedding model\n",
    "    RETURN\n",
    "    =====\n",
    "    \n",
    "    '''\n",
    "    if isinstance(text, str):\n",
    "        sent_tokens, text_input = preprocess_full_text(text, corpus, sentence_break = sentence_break)\n",
    "    else:\n",
    "        print(\"Input text must be a string\")\n",
    "        return\n",
    "        \n",
    "    avg_vec = []\n",
    "    \n",
    "    for sentence in text_input:\n",
    "\n",
    "        vectors = []\n",
    "        \n",
    "        for word in sentence:\n",
    "\n",
    "            try:\n",
    "                vectors.append(model[word])\n",
    "                \n",
    "            except KeyError:\n",
    "                print(f'{word} does not exist.')\n",
    "                continue\n",
    "                \n",
    "        avg = np.average(vectors, axis = 0)\n",
    "        avg_vec.append(avg)\n",
    "        \n",
    "    return sent_tokens, avg_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_names = symptoms.name.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only symptoms (adding a bit more details to help with vector averaging)\n",
    "symptoms = {'Spots': 'Spots, marks, dots, specks on skin', \n",
    "            'Freckles on armpit': 'Freckles, pigmentation, blotch, speck on armpit, underarm', \n",
    "            'Freckles on groin': 'Freckles, pigmentation, blotch, speck on groin, crotch, privates', \n",
    "            'Itching': 'Itching, tickle, tingle',\n",
    "            'Cognitive Difficulties': 'Cognitive, cerebral, intellectual challenges, Difficulties', \n",
    "            'Vision Changes': 'Vision, eyesight, perception changes, alteration', \n",
    "            'Fractures' : 'Broken bone, crack, misshapen limb or joint, fractures', \n",
    "            'Pain': 'pain, hurt, ache, sore, aching',\n",
    "            'Bowel or bladder control problems': 'bowel or bladder control problem, fecal incontinence, leaky stool, feces, loss of bowel control', \n",
    "            'Breathing problems': 'breathing problems, hard to breath, short of breath, respiratory difficulty',\n",
    "            'Problem with movement': 'problem with movement, loss of balance, unstable walking, muscle weakness, slow or reduced movements', \n",
    "            'Numbness': 'loss of sensation, numbness, feeling, tingling, pins-and-needles, numb', \n",
    "            'Learning difficulties': 'learning disabilities, difficulty reading, writing, math, challenges',\n",
    "            'Attention issues': 'attention issues, adhd, attention deficit, difficulty focusing, hard to follow instruction', \n",
    "            'Nosebleed': 'nosebleed, bleeding nose', \n",
    "            'Heart Problem': 'heart problem, pounding in the chest',\n",
    "            'High blood pressure': 'high blood pressure',\n",
    "            'Chewing or swallowing problems': 'difficulty chewing or swallowing, pain when swallowing, dysphagia, trouble or problems biting, eating or passing down the throat', \n",
    "            'Constipation': 'constipation, difficulty pooping, hard, dry, difficult bowel movement, painful stool', \n",
    "            'Poor weight gain': 'not gaining weight, not growing, failure to thrive, poor weight gain',\n",
    "            'Gastroesophageal reflux': 'digestive disorder, gastroesophageal reflux disease, gerd, acidic stomach juices, acid refulx', \n",
    "            'Anxiety disorder': 'anxiety disorder, extreme fear, worry, panic, attack, agoraphobia, social anxiety, phobias', \n",
    "            'Arthritis': 'arthritis, swelling and tenderness of joints, stiff joints, limbs, pain', \n",
    "            'Depression': 'depression, sad, hopelessness, strong fear, irritable, quiet, feeling blue, emotionally down',\n",
    "            'Difficulties with social interactions': 'difficulties with social interactions, weak social skills, talk too much, communication disorder, challenges making friends, awkward', \n",
    "            'Fatigue': 'fatigue, feeling extremely tired, weary, exhausted, drowsy, exertion, drain, wear out',\n",
    "            'Headaches or migraines': 'pain in the head, head hurting, headaches, migraines, pain around faces', \n",
    "            'Joint pain': 'joint pain, hurting, aching, sore or discomfort on joints like hip, shoulders, elbows, knees, wrists, angkles', \n",
    "            'Loose joints': 'loose joints, hypermobile, extreme or unusual range of motion on joints like hip, shoulders, elbows, knees, wrists, angkles', \n",
    "            'Muscle coordination issues': 'muscle coordination issues, ataxia, uncoordinated movement, difficulty writing or eating, or walking',\n",
    "            'Other mental health problems': 'other mental health problems like eating disorders, ptsd, psychosis', \n",
    "            'Seizures or epilepsy': 'seizure of epilepsy, loss of consciousness or awareness, jerking or twitching muscle movements, temporary confusions, epileptic seizure', \n",
    "            'Sleep disturbances': 'sleep disturbances, insomnia, sleep apnea, difficulty, challenges falling asleep or staying asleep']      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem to solve\n",
    "It needs to categorize other symptoms not-relevant to NF.  \n",
    "(e.g. \"high fever\" should not be categorized as \"high blood pressure\")  \n",
    "It also needs to weigh heavier on the word combo. \n",
    "(e.g. \"my head hurts\" should be closer to 'headache' than 'large head size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neurofibroma does not exist.\n",
      "neurofibroma does not exist.\n",
      "neurofibroma does not exist.\n",
      "neurofibroma does not exist.\n",
      "mpnst does not exist.\n",
      "moyamoya does not exist.\n"
     ]
    }
   ],
   "source": [
    "_, symptom_vectors = get_avg_vectors('. '.join(symptom_names), model, corpus, \n",
    "                                  sentence_break = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "symptom_vectors = dict(zip(symptom_names, symptom_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['symptom_vectors']"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(symptom_vectors, 'symptom_vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sentence, see how close they are to target symptoms\n",
    "\n",
    "def identify_symptom(text, symptom_vectors, model, corpus, threshold = 0.5):\n",
    "    '''\n",
    "    Find the closest symptom per sentences\n",
    "    '''\n",
    "    sent_tokens, avg_vec = get_avg_vectors(text, model, corpus)\n",
    "    reference = {} # {1: {sentence: identified symptom}}\n",
    "    pred_symptoms = {}\n",
    "\n",
    "    for i, sent_vec in enumerate(avg_vec): \n",
    "        \n",
    "        # for each sentence\n",
    "        max_ = threshold\n",
    "\n",
    "        for symptom, sym_vec in symptom_vectors.items():\n",
    "            # get cosine similarity\n",
    "            similarity =  1 - cosine(sent_vec, sym_vec)\n",
    "            # find the highest similarity\n",
    "            if similarity > max_:\n",
    "\n",
    "                max_ = similarity\n",
    "                max_symptom = symptom\n",
    "        \n",
    "        \n",
    "        if max_ > threshold:\n",
    "            if max_symptom in pred_symptoms: \n",
    "                \n",
    "                # if symptom already exists, update if similarity is higher\n",
    "                if max_ > pred_symptoms[max_symptom] : \n",
    "                    pred_symptoms[max_symptom] = max_\n",
    "\n",
    "            else: \n",
    "                # add symptom if it does not exist\n",
    "                pred_symptoms[max_symptom] = max_\n",
    "                \n",
    "        reference[i] = {sent_tokens[i] : max_symptom}\n",
    "\n",
    "    if pred_symptoms: \n",
    "        return [k for k, v in sorted(pred_symptoms.items(), key = lambda item: item[1])], reference        \n",
    "    else: \n",
    "        # Flag if no symptom was detected.\n",
    "        return ['NA'], reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vision changes',\n",
       " 'Numbness',\n",
       " 'Problem with movement',\n",
       " 'High blood pressure',\n",
       " 'Pain',\n",
       " 'Headaches or migraines']"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I have a high fever and headache. Things look a little strange. Pain on the left side of the body. Also felt a bit of numbness. It seems like I had hard time moving my limbs\"\n",
    "result, reference = identify_symptom(text, symptom_vectors, model, corpus)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'i have a high fever.': 'High blood pressure'},\n",
       " 1: {'headache.': 'Headaches or migraines'},\n",
       " 2: {'things look a little strange.': 'Vision changes'},\n",
       " 3: {'pain on the left side of the body.': 'Pain'},\n",
       " 4: {'also felt a bit of numbness.': 'Numbness'},\n",
       " 5: {'it seems like i had hard time moving my limbs': 'Problem with movement'}}"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_symptom_id(symptom_list, keys):\n",
    "    return [keys[keys.symptom == x].index[0] for x in symptom_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 25, 24, 21, 58]"
      ]
     },
     "execution_count": 927,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return symptom_id for this\n",
    "result_symptom_id = return_symptom_id(result, keys)\n",
    "result_symptom_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next iteration, we can present users with the select list of symptoms so they can provide a feedback as to how accurate our model is, then retrain based on their answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Testing using medical transcription data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = pd.read_csv('data/medical_transcription_samples.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4999 entries, 0 to 4998\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   description        4999 non-null   object\n",
      " 1   medical_specialty  4999 non-null   object\n",
      " 2   sample_name        4999 non-null   object\n",
      " 3   transcription      4966 non-null   object\n",
      " 4   keywords           3931 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 234.3+ KB\n"
     ]
    }
   ],
   "source": [
    "mt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing all transcriptions\n",
    "text_input = [preprocess(x) for x in mt.transcription]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# unnesting once\n",
    "text_input = list(chain(*text_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140476"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is when we train our own vector space using text_input preprocessed above\n",
    "#model = Word2Vec(sentences = text_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
